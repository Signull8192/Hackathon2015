{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re # for regex\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "# import matplotlib.pyplot as plt\n",
    "# get_ipython().magic('matplotlib inline')\n",
    "\n",
    "from chainer import cuda, Function, FunctionSet, gradient_check, Variable, optimizers\n",
    "import chainer.functions as F\n",
    "\n",
    "from dA import DenoisingAutoencoder\n",
    "from SdA import StackedDenoisingAutoencoder\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Params\n",
    "use_cuda = True\n",
    "\n",
    "batchsize = 100\n",
    "if use_cuda:\n",
    "    n_epoch_SdA = 30\n",
    "    n_epoch_fA = 50\n",
    "    n_epoch_last = 30\n",
    "else:\n",
    "    n_epoch_SdA = 5\n",
    "    n_epoch_fA = 10\n",
    "    n_epoch_last = 30\n",
    "\n",
    "if use_cuda:\n",
    "    path_imagedir = {'self':os.environ['HOME'] + '/Hevy/wba_hackathon/self_mit_fp_50x1000/', 'other':os.environ['HOME'] + '/Hevy/wba_hackathon/other_mit_fp_50x100/'}\n",
    "    n_dataset_self = 1000\n",
    "    n_dataset_other = 100\n",
    "else:\n",
    "    path_imagedir = {'self':os.environ['HOME'] + '/Hevy/wba_hackathon/self_mit_fp_50x100/', 'other':os.environ['HOME'] + '/Hevy/wba_hackathon/other_mit_fp_50x100/'}\n",
    "    n_dataset_self = 100\n",
    "    n_dataset_other = 100\n",
    "n_dataset = n_dataset_self + n_dataset_other\n",
    "\n",
    "size_image = [64, 64]\n",
    "scale = 4\n",
    "size_image[0]=size_image[0]/scale\n",
    "size_image[1]=size_image[1]/scale\n",
    "\n",
    "n_hold = 10\n",
    "n_cross = 1\n",
    "\n",
    "n_fA_node = 128\n",
    "n_loss_node = 48\n",
    "\n",
    "n_moveframe = 50\n",
    "n_oneframe = 5\n",
    "n_onemovie = int(n_moveframe / n_oneframe)\n",
    "if use_cuda:\n",
    "    n_hiddens = (18**2, 12**2, 6**2)\n",
    "else:\n",
    "    n_hiddens = (8**2, 2**2)\n",
    "\n",
    "num_images = n_dataset * n_moveframe\n",
    "num_movie = num_images / n_oneframe\n",
    "\n",
    "num_test_dataset = n_dataset // n_hold\n",
    "num_train_dataset = n_dataset - num_test_dataset\n",
    "num_test_movie =  num_movie // n_hold\n",
    "num_train_movie = num_movie - num_test_movie\n",
    "\n",
    "if use_cuda:\n",
    "    cuda.check_cuda_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## load images\n",
    "size = size_image[0]\n",
    "num_pximage = size**2\n",
    "num_pxmovie = n_oneframe * num_pximage\n",
    "\n",
    "# load images\n",
    "movies = np.zeros((n_dataset, n_moveframe, num_pximage), dtype=np.float32)\n",
    "i = 0\n",
    "for label in {'self', 'other'}:\n",
    "    for name in os.listdir(path_imagedir[label]):\n",
    "        if re.match( '.*png$', name ):\n",
    "            img = Image.open( os.path.join(path_imagedir[label], name) )\n",
    "            img.thumbnail( (size_image[0], size_image[1]) )\n",
    "            img = np.asarray(img, dtype=np.float32).mean(axis=2).T\n",
    "            movies[i//n_moveframe, i%n_moveframe, :] = np.reshape( img / 255.0, (1, -1) )\n",
    "            i = i+1\n",
    "\n",
    "## load json files\n",
    "joint_angles = [{}] * num_images\n",
    "i = 0\n",
    "for label in {'self', 'other'}:\n",
    "    for name in os.listdir(path_imagedir[label]):\n",
    "        if re.match( '.*json$', name ):\n",
    "            j = json.load( open(os.path.join(path_imagedir[label], name)) )\n",
    "            joint_angles[i] = j['joint_angle']\n",
    "            i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## setup ML values\n",
    "v_all = np.reshape(movies, (n_dataset, -1))\n",
    "v_all = utils.splitInputs(v_all, n_moveframe/n_oneframe)\n",
    "\n",
    "num_node_x = 8\n",
    "x_all = np.zeros((num_images, num_node_x), dtype=np.float32)\n",
    "for i in range(len(joint_angles)):\n",
    "    x_all[i][0:3] = [joint_angles[i]['left_shoulder']['y'],\\\n",
    "                     joint_angles[i]['left_shoulder']['p'],\\\n",
    "                     joint_angles[i]['left_shoulder']['r']]\n",
    "    x_all[i][3]   =  joint_angles[i]['left_elbow']['p']\n",
    "    x_all[i][4:7] = [joint_angles[i]['right_shoulder']['y'],\\\n",
    "                     joint_angles[i]['right_shoulder']['p'],\\\n",
    "                     joint_angles[i]['right_shoulder']['r']]\n",
    "    x_all[i][7]   =  joint_angles[i]['right_elbow']['p']\n",
    "\n",
    "x_all = x_all/180\n",
    "x_all = utils.bindInputs(x_all, n_moveframe)\n",
    "x_all = utils.splitInputs(x_all, n_moveframe/n_oneframe)\n",
    "\n",
    "# x_all[0:n_dataset, :, :] = np.random.rand(x_all.shape[-1])\n",
    "# v_all[0:n_dataset_self, :, :] = np.random.rand(v_all.shape[-1])\n",
    "# v_all[n_dataset_self:n_dataset, :, :] = np.random.rand(n_dataset_other, n_onemovie, v_all.shape[-1])\n",
    "\n",
    "# label 0:other, 1:self\n",
    "label_x = np.append( np.ones((n_dataset_self), dtype=np.int32), np.zeros((n_dataset_other), dtype=np.int32) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shuffle all data\n",
    "rng = np.random.RandomState(1234)\n",
    "indices = np.arange(n_dataset, dtype=np.int32)\n",
    "rng.shuffle(indices)\n",
    "v_all   = v_all[indices]\n",
    "x_all   = x_all[indices]\n",
    "label_x = label_x[indices]\n",
    "\n",
    "n_set = n_dataset / n_hold\n",
    "# split each data into 10 block\n",
    "v_s = np.split(v_all, n_set*np.r_[1:n_hold])\n",
    "x_s = np.split(x_all, n_set*np.r_[1:n_hold])\n",
    "label_x_s = np.split(label_x, n_set*np.r_[1:n_hold])\n",
    "\n",
    "num_layers= len(n_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward(x_data, y_data):\n",
    "    x = Variable(x_data); t = Variable(y_data)\n",
    "    h = F.sigmoid(model.l1(x))\n",
    "    y = model.l2(h)\n",
    "    return F.mean_squared_error(y, t), y\n",
    "\n",
    "def forwardLastML(x_data, y_data):\n",
    "    x = Variable(x_data); t = Variable(y_data)\n",
    "    h = F.sigmoid(model.l1(x))\n",
    "    y = model.l2(h)\n",
    "    return F.softmax_cross_entropy(y, t), y\n",
    "\n",
    "list_cross = []\n",
    "for i in range(n_cross):\n",
    "    # split test and train data\n",
    "    set_l = list(set(range(n_hold)).difference([i]))\n",
    "    v_train = np.empty(0, dtype=np.float32)\n",
    "    x_train = np.empty(0, dtype=np.float32)\n",
    "    label_train = np.empty(0, dtype=np.int32)\n",
    "    for i_set in range(n_hold-1):\n",
    "        v_train = utils.vstack_(v_train, v_s[set_l[i_set]])\n",
    "        x_train = utils.vstack_(x_train, x_s[set_l[i_set]])\n",
    "        label_train = utils.vstack_(label_train, label_x_s[set_l[i_set]])\n",
    "    \n",
    "    v_train = np.reshape(v_train, (num_train_movie, -1))\n",
    "    x_train = np.reshape(x_train, (num_train_movie, -1))\n",
    "    label_train = np.reshape(label_train, (num_train_dataset, -1))\n",
    "    v_test = np.reshape(v_s[i], (num_test_movie, -1))\n",
    "    x_test = np.reshape(x_s[i], (num_test_movie, -1))\n",
    "    label_test = label_x_s[i]\n",
    "    \n",
    "    # create SdA\n",
    "    sda = StackedDenoisingAutoencoder(num_pxmovie, n_hiddens, n_epoch=n_epoch_SdA, use_cuda=use_cuda)\n",
    "    sda.train(v_train)\n",
    "    \n",
    "    # split test and train data\n",
    "    y_train_each = sda.predict(v_train, bAllLayer=True)\n",
    "    y_test_each = sda.predict(v_test, bAllLayer=True)\n",
    "    \n",
    "    list_layer = []\n",
    "    for j in range(num_layers):\n",
    "        y_train  = y_train_each[j]\n",
    "        y_test   = y_test_each[j]\n",
    "        \n",
    "        # separate x&y into other and self\n",
    "        x_test_split = [np.empty(0,dtype=np.float32), np.empty(0,dtype=np.float32)]\n",
    "        y_test_split = [np.empty(0,dtype=np.float32), np.empty(0,dtype=np.float32)]\n",
    "        for i_test in range(int(num_test_movie)):\n",
    "            label = label_test[i_test//n_onemovie]\n",
    "            x_test_split[label] = utils.vstack_(x_test_split[label], x_test[i_test])\n",
    "            y_test_split[label] = utils.vstack_(y_test_split[label], y_test[i_test])\n",
    "        \n",
    "        # train with only self body\n",
    "        num_train_self = 0\n",
    "        x_train_split = np.empty(0,dtype=np.float32)\n",
    "        y_train_split = np.empty(0,dtype=np.float32)\n",
    "        for i_train in range(int(num_train_movie)):\n",
    "            if label_train[i_train//n_onemovie]==1:\n",
    "                x_train_split = utils.vstack_(x_train_split, x_train[i_train])\n",
    "                y_train_split = utils.vstack_(y_train_split, y_train[i_train])\n",
    "                num_train_self = num_train_self + 1\n",
    "        \n",
    "        # f(x->y)\n",
    "        model = FunctionSet(\n",
    "            l1 = F.Linear(num_node_x*n_oneframe, n_fA_node),\n",
    "            l2 = F.Linear(n_fA_node, n_hiddens[j])\n",
    "        )\n",
    "        optimizer = optimizers.SGD()\n",
    "        optimizer.setup(model.collect_parameters())\n",
    "        \n",
    "        dic = {'loss':{}, 'hist':{}, 'lastpredict':{}}\n",
    "        dic['loss'] = {'self':np.empty(0,dtype=np.float32), 'other':np.empty(0,dtype=np.float32)}\n",
    "        for epoch in range(n_epoch_fA):\n",
    "            indexes = np.random.permutation(int(num_train_self))\n",
    "            sum_loss = 0\n",
    "            for k in range(0, num_train_self, batchsize):\n",
    "                x_batch = x_train_split[indexes[k : k + batchsize]]\n",
    "                y_batch = y_train_split[indexes[k : k + batchsize]]\n",
    "                optimizer.zero_grads()\n",
    "                loss, output = forward(x_batch, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.update()\n",
    "                sum_loss = sum_loss+loss.data*batchsize\n",
    "            print('fA: epoch:'+str(epoch)+' loss:' + str(sum_loss/num_train_movie))\n",
    "            \n",
    "            # test\n",
    "            loss, output = forward(x_test_split[1], y_test_split[1])\n",
    "            dic['loss']['self'] = utils.vstack_(dic['loss']['self'], loss.data)\n",
    "            loss, output = forward(x_test_split[0], y_test_split[0])\n",
    "            dic['loss']['other'] = utils.vstack_(dic['loss']['other'], loss.data)\n",
    "            print('test loss:' + str(loss.data))\n",
    "        \n",
    "        dic['hist'] = {'self':np.empty(0, dtype=np.float32), 'other':np.empty(0, dtype=np.float32)}\n",
    "        for i_test in range((x_test_split[1].shape[0])):\n",
    "            loss, output = forward(x_test_split[1][i_test][None], y_test_split[1][i_test][None]) # [8,][None] -> [1,8]\n",
    "            dic['hist']['self'] = utils.vstack_(dic['hist']['self'], loss.data)\n",
    "        for i_test in range(x_test_split[0].shape[0]):\n",
    "            loss, output = forward(x_test_split[0][i_test][None], y_test_split[0][i_test][None])\n",
    "            dic['hist']['other'] = utils.vstack_(dic['hist']['other'], loss.data)\n",
    "        \n",
    "        # loss => self or other\n",
    "        loss_train = np.zeros((num_train_dataset, n_onemovie), dtype=np.float32)\n",
    "        for i_train in range(num_train_dataset):\n",
    "            for i_movie in range(n_onemovie):\n",
    "                loss, output = forward(x_train[i_train*n_onemovie+i_movie][None], y_train[i_train*n_onemovie+i_movie][None])\n",
    "                loss_train[i_train, i_movie] = loss.data\n",
    "        loss_test = np.zeros((num_test_dataset, n_onemovie), dtype=np.float32)\n",
    "        for i_test in range(num_test_dataset):\n",
    "            for i_movie in range(n_onemovie):\n",
    "                loss, output = forward(x_test[i_test*n_onemovie+i_movie][None], y_test[i_test*n_onemovie+i_movie][None])\n",
    "                loss_test[i_test, i_movie] = loss.data\n",
    "                \n",
    "        m = np.r_[loss_train, loss_test].max()\n",
    "        loss_train = loss_train / m\n",
    "        loss_test = loss_test / m\n",
    "        \n",
    "        model = FunctionSet(\n",
    "            l1 = F.Linear(n_onemovie, n_loss_node),\n",
    "            l2 = F.Linear(n_loss_node, 2)\n",
    "        )\n",
    "        optimizer = optimizers.SGD()\n",
    "        optimizer.setup(model.collect_parameters())\n",
    "        \n",
    "        for epoch in range(n_epoch_last):\n",
    "            indexes = np.random.permutation(int(num_train_dataset))\n",
    "            sum_loss = 0\n",
    "            for k in range(0, int(num_train_dataset), batchsize):\n",
    "                x_batch = loss_train[indexes[k : k + batchsize]]\n",
    "                y_batch = label_train[indexes[k : k + batchsize]].ravel()\n",
    "                optimizer.zero_grads()\n",
    "                loss, output = forwardLastML(x_batch, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.update()\n",
    "                sum_loss = sum_loss+loss.data*batchsize\n",
    "            print('LastML: epoch:'+str(epoch)+' loss:' + str(sum_loss/num_train_dataset))\n",
    "        \n",
    "        dic['lastpredict']['label'] = label_test.reshape(-1)\n",
    "        dic['lastpredict']['pedict'] = np.empty(0, dtype=np.int)\n",
    "        dic['lastpredict']['output'] = np.empty(0, dtype=np.float)\n",
    "        for i_test in range(num_test_dataset):\n",
    "            loss, output = forwardLastML(loss_test[i_test][None], label_test[i_test].ravel())\n",
    "            dic['lastpredict']['output'] = utils.vstack_(dic['lastpredict']['output'], output.data)\n",
    "            if output.data[0,0] > output.data[0,1]:\n",
    "                dic['lastpredict']['pedict'] = utils.vstack_(dic['lastpredict']['pedict'], 0)\n",
    "            else:\n",
    "                dic['lastpredict']['pedict'] = utils.vstack_(dic['lastpredict']['pedict'], 1)\n",
    "        \n",
    "        list_layer.append(dic)\n",
    "        \n",
    "    list_cross.append(list_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "f = open('save.dump', 'wb')\n",
    "pickle.dump(list_cross, f)\n",
    "\n",
    "print('finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
