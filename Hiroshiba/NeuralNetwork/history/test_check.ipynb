{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re # for regex\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import importlib\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import pycuda.autoinit\n",
    "\n",
    "from chainer import cuda, Function, FunctionSet, gradient_check, Variable, optimizers\n",
    "import chainer.functions as F\n",
    "\n",
    "from dA import DenoisingAutoencoder\n",
    "from SdA import StackedDenoisingAutoencoder\n",
    "from CdA import ConvolutionalDenoisingAutoencoder\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## load images\n",
    "path_imagedir = '/Users/Hiho/Downloads/mit_body_v2'\n",
    "\n",
    "# count up\n",
    "num_images = 0\n",
    "for name in os.listdir(path_imagedir):\n",
    "    if re.match( '.*png$', name ):\n",
    "        num_images = num_images+1\n",
    "        \n",
    "# get image size\n",
    "for name in os.listdir(path_imagedir):\n",
    "    if re.match( '.*png$', name ):\n",
    "        img = Image.open( os.path.join(path_imagedir, name) )\n",
    "        size_image = img.size\n",
    "        break\n",
    "num_pximage = size_image[0]*size_image[1]\n",
    "\n",
    "# laod images\n",
    "imgs = np.zeros((num_images, num_pximage), dtype=np.float32)\n",
    "i=0\n",
    "for name in os.listdir(path_imagedir):\n",
    "    if re.match( '.*png$', name ):\n",
    "        img = Image.open( os.path.join(path_imagedir, name) )\n",
    "        img = np.asarray(img, dtype=np.uint8).T\n",
    "        imgs[i,:] = np.reshape( np.mean(img, axis=0), (1, -1) ).astype(np.float32) / 255\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## make movie\n",
    "num_frame = 5\n",
    "num_movies = num_images - num_frame + 1\n",
    "num_pxmovie = num_pximage*num_frame\n",
    "movies = np.zeros((num_movies, num_pxmovie), dtype=np.float32)\n",
    "for i in range(num_movies):\n",
    "    movies[i,:] = np.reshape( imgs[i:i+5,:], (1, -1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## load json files\n",
    "i=0\n",
    "true_poses = [{}] * num_images\n",
    "joint_angles = [{}] * num_images\n",
    "for name in os.listdir(path_imagedir):\n",
    "    if re.match( '.*json$', name ):\n",
    "        j = json.load( open(os.path.join(path_imagedir, name)) )\n",
    "        true_poses[i] = j['true_position']\n",
    "        joint_angles[i] = j['joint_angle']\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## setup ML values\n",
    "num_test =  num_movies // 40\n",
    "num_train = num_movies - num_test\n",
    "v_all = movies.copy()\n",
    "\n",
    "num_node_tp = 9\n",
    "tp_all = np.zeros((num_movies, num_node_tp), dtype=np.float32)\n",
    "for i in range(num_movies):\n",
    "    tp = true_poses[i+num_frame-1]\n",
    "    tp_all[i][0:3] = [tp['right_elbow']['x'], tp['right_elbow']['y'], tp['right_elbow']['z']]\n",
    "    tp_all[i][3:6] = [tp['right_shoulder']['x'], tp['right_shoulder']['y'], tp['right_shoulder']['z']]\n",
    "    tp_all[i][6:9] = [tp['right_hand']['x'], tp['right_hand']['y'], tp['right_hand']['z']]\n",
    "    \n",
    "num_node_xA = 4\n",
    "xA_all = np.zeros((num_movies, num_node_xA), dtype=np.float32)\n",
    "for i in range(num_movies):\n",
    "    xA = joint_angles[i+num_frame-1]\n",
    "    xA_all[i][0:3] = [xA['right_shoulder']['y'], xA['right_shoulder']['p'], xA['right_shoulder']['r']]\n",
    "    xA_all[i][3] = xA['right_elbow']['p']\n",
    "xA_all = xA_all/360\n",
    "\n",
    "# shuffle all data\n",
    "rng = np.random.RandomState(1234)\n",
    "indices = np.arange(num_movies)\n",
    "rng.shuffle(indices)\n",
    "v_all = v_all[indices]\n",
    "tp_all = tp_all[indices]\n",
    "\n",
    "# split test and train data\n",
    "v_train, v_test = np.split(v_all, [num_train])\n",
    "tp_train, tp_test = np.split(tp_all, [num_train])\n",
    "xA_train, xA_test = np.split(xA_all, [num_train])\n",
    "\n",
    "batchsize = 100\n",
    "n_epoch = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create SdA\n",
    "n_hiddens = (12**2*num_frame, 6**2*num_frame)\n",
    "sda = StackedDenoisingAutoencoder(num_pxmovie, n_hiddens)\n",
    "sda.train(v_all, n_epoch=n_epoch)\n",
    "sda.save('history', n_hiddens, n_epoch, batchsize)\n",
    "# sda.load('history/SdA_layer(576, 64)_epoch300.pkl')\n",
    "\n",
    "# split test and train data\n",
    "yA_each = sda.predict(v_all, bAllLayer=True)\n",
    "yA_all = yA_each[-1]\n",
    "# yA_hidden1_all = yA_each[0]\n",
    "yA_train, yA_test = np.split(yA_all, [num_train])\n",
    "\n",
    "# check output histgram\n",
    "dummy = plt.hist(np.reshape(yA_all, (-1, 1)), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## draw weight\n",
    "num_show = 4\n",
    "for i_layer in range(len(n_hiddens)):\n",
    "    for i in range(num_show):\n",
    "        for i_frame in range(num_frame):\n",
    "            plt.subplot(len(n_hiddens)*num_frame, num_show, num_show*(num_frame*i_layer+i_frame)+i+1)\n",
    "            iw_s = num_pximage*i_frame\n",
    "            iw_e = num_pximage*(i_frame+1)\n",
    "            draw_weight( sda.SdA[i_layer].model.encode.W[i][iw_s:iw_e], (math.sqrt(sda.n_nodes[i_layer]/num_frame), math.sqrt(sda.n_nodes[i_layer]/num_frame)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check true position\n",
    "model = FunctionSet(\n",
    "#     l1 = F.Linear(n_hiddens[-1], 50),\n",
    "#     l2 = F.Linear(50, num_node_tp),\n",
    "    l = F.Linear(n_hiddens[-1], num_node_tp),\n",
    ")\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model.collect_parameters())\n",
    "\n",
    "def forward(x_data, y_data):\n",
    "    x = Variable(x_data); t = Variable(y_data)\n",
    "#     h = F.relu(model.l1(x))\n",
    "    y = model.l(x)\n",
    "    return F.mean_squared_error(y, t), y\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    indexes = np.random.permutation(num_train)\n",
    "    sum_loss = 0\n",
    "    for i in range(0, num_train, batchsize):\n",
    "        x_batch = yA_train[indexes[i : i + batchsize]]\n",
    "        y_batch = tp_train[indexes[i : i + batchsize]]\n",
    "        optimizer.zero_grads()\n",
    "        loss, output = forward(x_batch, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        sum_loss = sum_loss+loss.data*batchsize\n",
    "    print('epoch:'+str(epoch)+' loss:' + str(sum_loss/num_train))\n",
    "    \n",
    "# test\n",
    "loss, output = forward(yA_test, tp_test)\n",
    "print('test loss:' + str(loss.data))\n",
    "\n",
    "for i_check in range(0, num_test, math.floor(num_test/8)):\n",
    "    print(i_check)\n",
    "    print( \"true : %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f\" % (tp_test[i_check][0], tp_test[i_check][1], tp_test[i_check][2], tp_test[i_check][3], tp_test[i_check][4], tp_test[i_check][5], tp_test[i_check][6], tp_test[i_check][7], tp_test[i_check][8] ))\n",
    "    print( \"predicted : %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f\" % (output.data[i_check][0], output.data[i_check][1], output.data[i_check][2], output.data[i_check][3], output.data[i_check][4], output.data[i_check][5], output.data[i_check][6], output.data[i_check][7], output.data[i_check][8] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fA(xA->yA)\n",
    "model = FunctionSet(\n",
    "    l1 = F.Linear(num_node_xA, 50),\n",
    "    l2 = F.Linear(50, n_hiddens[-1]),\n",
    ")\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model.collect_parameters())\n",
    "\n",
    "def forward(x_data, y_data):\n",
    "    x = Variable(x_data); t = Variable(y_data)\n",
    "    h = F.sigmoid(model.l1(x))\n",
    "    y = model.l2(h)\n",
    "    return F.mean_squared_error(y, t), y\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    indexes = np.random.permutation(num_images)\n",
    "    sum_loss = 0\n",
    "    for i in range(0, num_train, batchsize):\n",
    "        x_batch = xA_all[indexes[i : i + batchsize]]\n",
    "        y_batch = yA_all[indexes[i : i + batchsize]]\n",
    "        optimizer.zero_grads()\n",
    "        loss, output = forward(x_batch, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        sum_loss = sum_loss+loss.data*batchsize\n",
    "    print('epoch:'+str(epoch)+' loss:' + str(sum_loss/num_train))\n",
    "    \n",
    "# test\n",
    "loss, output = forward(xA_test, yA_test)\n",
    "print('test loss:' + str(loss.data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
